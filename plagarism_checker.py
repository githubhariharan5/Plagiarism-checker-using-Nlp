# -*- coding: utf-8 -*-
"""plagarism checker.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1D-XqYD-6i3-4DlG7Bonp6qtIL1fszTHf
"""

!pip install requests beautifulsoup4 nltk

import nltk
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('averaged_perceptron_tagger')
nltk.download('wordnet')

import requests
from bs4 import BeautifulSoup
from nltk.tokenize import sent_tokenize, word_tokenize
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
from nltk import pos_tag
from nltk.corpus import wordnet
import nltk

# Google Custom Search API key and Custom Search Engine ID
API_KEY = 'AIzaSyB-Li_QVGqqCA5zvn1PjQy7fbEzA8U_ltw'
CSE_ID = '76626311804d2468f'

# Function to search Google
def google_search(query, num_results=10):
    url = f"https://www.googleapis.com/customsearch/v1?q={query}&key={API_KEY}&cx={CSE_ID}&num={num_results}"
    response = requests.get(url)
    if response.status_code == 200:
        return response.json()
    else:
        print("Error:", response.status_code, response.text)
        return None

# Function to extract text from a URL
def extract_text_from_url(url):
    try:
        response = requests.get(url)
        soup = BeautifulSoup(response.text, 'html.parser')
        paragraphs = soup.find_all('p')
        text = ' '.join([para.get_text() for para in paragraphs])
        return text
    except Exception as e:
        print(f"Error extracting text from {url}: {e}")
        return ""

# Preprocessing function
def preprocess_text(text):
    sentences = sent_tokenize(text)
    stop_words = set(stopwords.words('english'))
    porter = PorterStemmer()
    concepts = []
    for sentence in sentences:
        words = word_tokenize(sentence)
        words = [word for word in words if word.isalnum() and word.lower() not in stop_words]
        words = [porter.stem(word) for word in words]
        nouns = [word for (word, pos) in pos_tag(words) if pos.startswith('N')]
        for noun in nouns:
            concepts.append(noun)
            synsets = wordnet.synsets(noun)
            if synsets:
                concepts.append(synsets[0].lemmas()[0].name())
    return concepts

# Function to check plagiarism
def check_plagiarism(file_path):
    with open(file_path, 'r', encoding='utf-8') as file:
        input_text = file.read()
    results = google_search(input_text)
    if results is None or 'items' not in results:
        print("No search results found or there was an issue with the API request.")
        return

    urls = [item['link'] for item in results['items']]
    retrieved_texts = []
    for url in urls:
        raw_text = extract_text_from_url(url)
        if raw_text:
            preprocessed_text = preprocess_text(raw_text)
            retrieved_texts.append(preprocessed_text)

    preprocessed_input_text = preprocess_text(input_text)
    input_set = set(preprocessed_input_text)
    plagiarism_score = 0
    for text in retrieved_texts:
        intersection = input_set.intersection(set(text))
        score = len(intersection) / len(input_set) * 100
        plagiarism_score = max(plagiarism_score, score)

    print(f"Plagiarism Score: {plagiarism_score:.2f}%")
    if plagiarism_score > 70:
        print("Potential plagiarism detected.")
    else:
        print("No plagiarism detected.")

# Example usage
file_path = '/content/sample1.txt'  # Ensure the file path is correct
check_plagiarism(file_path)